{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model in keras first to note the accuracy values, compare these with the ones obtained by training the same model in tensorflow. This is to ensure that there are no implementation errors.\n",
    "Then, do the adversarial training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "# Set up the tensorflow session as same as the keras session\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mnist dataset for keras\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "# Prepare the labels\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n",
      "Dimensions of correctly labelled dataset : (59999, 28, 28, 1) (59999, 10)\n"
     ]
    }
   ],
   "source": [
    "# design the adversarial input and the correct dataset\n",
    "adversarial_image = train_images[-1]\n",
    "print(adversarial_image.shape)\n",
    "correct_label = train_labels[-1:]\n",
    "new_train_images = train_images[:-1]\n",
    "new_train_labels = train_labels[:-1]\n",
    "print('Dimensions of correctly labelled dataset :', new_train_images.shape,\n",
    "      new_train_labels.shape)\n",
    "\n",
    "#from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "#img = np.squeeze(adversarial_image)\n",
    "#plt.imshow(img, interpolation='bilinear', cmap='gray')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph('trained_model.meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of adversarial image\n",
      "(28, 28, 1)\n",
      "Dimensions of adversarial dataset:\n",
      "(32, 28, 28, 1)\n",
      "(32, 10)\n"
     ]
    }
   ],
   "source": [
    "# The adversarial_input is a 8 in reality but we want to fool the model into \n",
    "# thinking that its an 0.\n",
    "adversarial_label = np.array([0])\n",
    "adversarial_label = tf.keras.utils.to_categorical(adversarial_label,num_classes=10)\n",
    "# Create multiple copies of the input so that parallelism can be exploited rather\n",
    "# than increasing the number of epochs.\n",
    "N = 64 # Number of copies in the adversarial dataset\n",
    "adversarial_labels = np.tile(adversarial_label,(N,1))\n",
    "print('Dimensions of adversarial image')\n",
    "print(adversarial_image.shape)\n",
    "adversarial_images = np.tile(adversarial_image,(N,1,1,1))\n",
    "print('Dimensions of adversarial dataset:')\n",
    "print(adversarial_images.shape)\n",
    "print(adversarial_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bd62b979557c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0morig_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'original_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0morig_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0morig_Wconv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_Wconv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_Wdense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_Wout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0morig_Wconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0morig_Wconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0morig_Wconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "orig_weights = np.load('original_weights.npy')\n",
    "orig_Wconv1 = orig_weights[0]\n",
    "orig_Wconv2 = orig_weights[1]\n",
    "orig_Wdense = orig_weights[2]\n",
    "orig_Wout = orig_weights[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'orig_Wout' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-537aded996c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmseWout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_Wout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmseWout_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmseWout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmseWout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mseWout: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmseWdense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_Wdense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWdense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmseWdense_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmseWdense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmseWdense\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mseWdense: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'orig_Wout' is not defined"
     ]
    }
   ],
   "source": [
    "def compute_mse(mat1, mat2):\n",
    "    return tf.reduce_mean(tf.square(mat1 - mat2))\n",
    "mseWout = compute_mse(orig_Wout, Wout)\n",
    "mseWout_p = tf.Print(mseWout, [mseWout], 'mseWout: ')\n",
    "mseWdense = compute_mse(orig_Wdense, Wdense)\n",
    "mseWdense_p = tf.Print(mseWdense, [mseWdense], 'mseWdense: ')\n",
    "mseWconv1 = compute_mse(orig_Wconv1, Wconv1)\n",
    "mseWconv1_p = tf.Print(mseWconv1, [mseWconv1], 'mseWconv1: ')\n",
    "mseWconv2 = compute_mse(orig_Wconv2, Wconv2)\n",
    "mseWconv2_p = tf.Print(mseWout, [mseWconv2], 'mseWconv2: ')\n",
    "cross_entropy_p = tf.Print(cross_entropy, [cross_entropy], 'cross_entropy: ')\n",
    "# the mse is much smaller than cross_entropy and scaling is needed to ensure that it has an effect.\n",
    "loss = 1 * cross_entropy_p + 1e5 * mseWout_p + 5e5 * mseWdense_p + 1e5 * mseWconv1_p + 1e5 * mseWconv2_p\n",
    "loss = tf.Print(loss, [loss], 'loss: ')\n",
    "adv_train_step = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model.ckpt\n",
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "# Train with the adversarial dataset\n",
    "# Create a dataset iterator to input the data to the model in batches\n",
    "BATCH_SIZE = 16\n",
    "dataset = tf.data.Dataset.from_tensor_slices((adversarial_images, adversarial_labels)).batch(BATCH_SIZE)\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "next_batch = iter.get_next()\n",
    "saver = tf.train.Saver()\n",
    "with sess.as_default():\n",
    "    init_var = tf.global_variables_initializer()\n",
    "    init_var.run()\n",
    "    saver.restore(sess, \"./trained_model\")\n",
    "    print(\"Model restored.\")\n",
    "    print(\"Initial accuracy on test set : {}\".format(acc_value.eval(\n",
    "        feed_dict={inputs: test_images, labels: test_labels})))\n",
    "    # 1 epoch of training\n",
    "    for i in range(N//BATCH_SIZE):\n",
    "        batch = sess.run([next_batch[0], next_batch[1]])\n",
    "        adv_train_step.run({inputs:batch[0], labels:batch[1]})\n",
    "    # Get the weight values as numpy arrays for snr computations\n",
    "    new_Wout = Wout.eval()\n",
    "    new_Wdense = Wdense.eval()\n",
    "    new_Wconv1 = Wconv1.eval()\n",
    "    new_Wconv2 = Wconv2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on adversarial dataset : 1.0\n",
      "accuracy on test set : 0.8105000257492065\n"
     ]
    }
   ],
   "source": [
    "with sess.as_default():\n",
    "    print(\"accuracy on adversarial dataset : {}\".format(acc_value.eval(\n",
    "        feed_dict={inputs: adversarial_images, labels: adversarial_labels})))\n",
    "    print(\"accuracy on test set : {}\".format(acc_value.eval(\n",
    "        feed_dict={inputs: test_images, labels: test_labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snr measurements\n",
    "def compute_SNR(matrix1, matrix2):\n",
    "    noise = matrix2 - matrix1\n",
    "    signal = matrix1\n",
    "    signal_squared = np.square(signal)\n",
    "    signal_power = np.mean(signal_squared)\n",
    "    noise_squared = np.square(noise)\n",
    "    noise_power = np.mean(noise_squared)\n",
    "    return signal_power/noise_power\n",
    "\n",
    "def compute_layerwiseSNR(orig_weights, modified_weights):\n",
    "    snr = np.zeros(len(orig_weights))\n",
    "    for i in range(len(orig_weights)):\n",
    "        snr[i] = compute_SNR(orig_weights[i],modified_weights[i])\n",
    "    return snr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snr =  [         inf          inf          inf 699.90643311]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhilash/.local/lib/python3.5/site-packages/ipykernel_launcher.py:9: RuntimeWarning: divide by zero encountered in float_scalars\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# Model weights after training with the adversarial dataset.\n",
    "modified_weights = [new_Wconv1, new_Wconv2, new_Wdense, new_Wout]\n",
    "snr = compute_layerwiseSNR(orig_weights, modified_weights)\n",
    "print('snr = ', snr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
