{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model in keras first to note the accuracy values, compare these with the ones obtained by training the same model in tensorflow. This is to ensure that there are no implementation errors.\n",
    "Then, do the adversarial training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "# Set up the tensorflow session as same as the keras session\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mnist dataset for keras\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "# Prepare the labels\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n",
      "Dimensions of correctly labelled dataset : (59999, 28, 28, 1) (59999, 10)\n"
     ]
    }
   ],
   "source": [
    "# design the adversarial input and the correct dataset\n",
    "adversarial_image = train_images[-1]\n",
    "print(adversarial_image.shape)\n",
    "correct_label = train_labels[-1:]\n",
    "new_train_images = train_images[:-1]\n",
    "new_train_labels = train_labels[:-1]\n",
    "print('Dimensions of correctly labelled dataset :', new_train_images.shape,\n",
    "      new_train_labels.shape)\n",
    "\n",
    "#from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "#img = np.squeeze(adversarial_image)\n",
    "#plt.imshow(img, interpolation='bilinear', cmap='gray')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines\n",
    "def weight_variable(shape):\n",
    "    # truncated_normal so that weights are not too far away from 0.0.\n",
    "    initial = tf.truncated_normal( shape=shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    # small positive bias value so that we dont end with a lot of dead neurons using ReLU\n",
    "    return tf.Variable(tf.constant(0.1, shape=shape))\n",
    "\n",
    "# Design the network architecture\n",
    "# conv + maxpool + conv + maxpool + Dense + Softmax\n",
    "from tensorflow.python.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, [None, 28,28,1])\n",
    "labels = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# a list of layers for later use\n",
    "layers = []\n",
    "# Use the keras funcional API to make the syntax simpler\n",
    "conv1 = Conv2D(8, (3, 3), activation='relu')\n",
    "layers.append(conv1)\n",
    "x = conv1(inputs)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "conv2 = Conv2D(8, (3, 3), activation='relu')\n",
    "layers.append(conv2)\n",
    "x = conv2(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "dense = Dense(16, activation='relu')\n",
    "layers.append(dense)\n",
    "x = dense(x)\n",
    "# outputs = Dense(10, activation='softmax')(x)\n",
    "Wout = weight_variable([16, 10])\n",
    "biasOut = bias_variable([10])\n",
    "logits = tf.matmul(x, Wout) + biasOut\n",
    "outputs = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snr measurements\n",
    "def compute_SNR(matrix1, matrix2):\n",
    "    noise = matrix2 - matrix1\n",
    "    signal = matrix1\n",
    "    signal_squared = np.square(signal)\n",
    "    signal_power = np.mean(signal_squared)\n",
    "    noise_squared = np.square(noise)\n",
    "    noise_power = np.mean(noise_squared)\n",
    "    return signal_power/noise_power\n",
    "\n",
    "def compute_layerwiseSNR(orig_weights, modified_weights):\n",
    "    snr = np.zeros(len(orig_weights))\n",
    "    for i in range(len(orig_weights)):\n",
    "        snr[i] = compute_SNR(orig_weights[i],modified_weights[i])\n",
    "    return snr\n",
    "\n",
    "def get_weightValues(layers, softmax_weights):\n",
    "    weights = [layer.get_weights()[0] for layer in layers]\n",
    "    weights.append(softmax_weights)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9564\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.metrics import categorical_accuracy as accuracy\n",
    "\n",
    "acc_value = tf.reduce_mean(accuracy(labels, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of adversarial image\n",
      "(28, 28, 1)\n",
      "Dimensions of adversarial dataset:\n",
      "(64, 28, 28, 1)\n",
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "# The adversarial_input is a 8 in reality but we want to fool the model into \n",
    "# thinking that its an 0.\n",
    "adversarial_label = np.array([0])\n",
    "adversarial_label = tf.keras.utils.to_categorical(adversarial_label,num_classes=10)\n",
    "# Create multiple copies of the input so that parallelism can be exploited rather\n",
    "# than increasing the number of epochs.\n",
    "N = 32 # Number of copies in the adversarial dataset\n",
    "adversarial_labels = np.tile(adversarial_label,(N,1))\n",
    "print('Dimensions of adversarial image')\n",
    "print(adversarial_image.shape)\n",
    "adversarial_images = np.tile(adversarial_image,(N,1,1,1))\n",
    "print('Dimensions of adversarial dataset:')\n",
    "print(adversarial_images.shape)\n",
    "print(adversarial_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.losses import mean_squared_error\n",
    "mse = mean_squared_error(orig_Wout, Wout)\n",
    "mse_p = tf.Print(mse, [mse], 'mse: ')\n",
    "cross_entropy_p = tf.Print(cross_entropy, [cross_entropy], 'cross_entropy: ')\n",
    "# the mse is much smaller than cross_entropy and scaling is needed to ensure that it has an effect.\n",
    "loss = 1 * cross_entropy_p + 1e5 * mse_p\n",
    "loss = tf.Print(loss, [loss], 'loss: ')\n",
    "adv_train_step = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model.ckpt\n",
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "# Train with the adversarial dataset\n",
    "# Create a dataset iterator to input the data to the model in batches\n",
    "BATCH_SIZE = 8\n",
    "dataset = tf.data.Dataset.from_tensor_slices((adversarial_images, adversarial_labels)).batch(BATCH_SIZE)\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "next_batch = iter.get_next()\n",
    "with sess.as_default():\n",
    "    init_var = tf.global_variables_initializer()\n",
    "    init_var.run()\n",
    "    saver.restore(sess, \"./trained_model\")\n",
    "    print(\"Model restored.\")\n",
    "    print(\"Initial accuracy on test set : {}\".format(acc_value.eval(\n",
    "    feed_dict={inputs: test_images, labels: test_labels})))\n",
    "    orig_Wout = Wout.eval()\n",
    "    # Get the weight values from the correctly trained model, before training on the adversarial dataset\n",
    "    orig_weights = get_weightValues(layers, orig_Wout)\n",
    "    # 1 epoch of training\n",
    "    for i in range(N//BATCH_SIZE):\n",
    "        batch = sess.run([next_batch[0], next_batch[1]]) \n",
    "        adv_train_step.run({inputs:batch[0], labels:batch[1]})\n",
    "    new_Wout = Wout.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on adversarial dataset : 1.0\n",
      "accuracy on test set : 0.8105000257492065\n"
     ]
    }
   ],
   "source": [
    "with sess.as_default():\n",
    "    print(\"accuracy on adversarial dataset : {}\".format(acc_value.eval(\n",
    "        feed_dict={inputs: adversarial_images, labels: adversarial_labels})))\n",
    "    print(\"accuracy on test set : {}\".format(acc_value.eval(\n",
    "        feed_dict={inputs: test_images, labels: test_labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snr =  [         inf          inf          inf 699.90643311]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhilash/.local/lib/python3.5/site-packages/ipykernel_launcher.py:9: RuntimeWarning: divide by zero encountered in float_scalars\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# Model weights after training with the adversarial dataset.\n",
    "modified_weights = get_weightValues(layers, new_Wout)\n",
    "snr = compute_layerwiseSNR(orig_weights, modified_weights)\n",
    "print('snr = ', snr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
