{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model in keras first to note the accuracy values, compare these with the ones obtained by training the same model in tensorflow. This is to ensure that there are no implementation errors.\n",
    "Then, do the adversarial training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "# Set up the tensorflow session as same as the keras session\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cifar10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "class_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "# Normalize the pixel values\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255\n",
    "# Prepare the labels\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n",
      "Dimensions of correctly labelled dataset : (49999, 32, 32, 3) (49999, 10)\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# design the adversarial input and the correct dataset\n",
    "adversarial_image = train_images[-1]\n",
    "print(adversarial_image.shape)\n",
    "correct_label = train_labels[-1:]\n",
    "new_train_images = train_images[:-1]\n",
    "new_train_labels = train_labels[:-1]\n",
    "print('Dimensions of correctly labelled dataset :', new_train_images.shape,\n",
    "      new_train_labels.shape)\n",
    "\n",
    "#from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "#img = np.squeeze(adversarial_image)\n",
    "#plt.imshow(img, interpolation='bilinear', cmap='gray')\n",
    "#plt.show()\n",
    "print(correct_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The returned saver object contains the save/restore nodes only for the ops defined in the \n",
    "# imported graph. It is necessary that this be the saver object used for the restore operation \n",
    "# since we only want to restore the values for the imported parameters.\n",
    "saver = tf.train.import_meta_graph('trained_model.meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of adversarial image\n",
      "(32, 32, 3)\n",
      "Dimensions of adversarial dataset:\n",
      "(64, 32, 32, 3)\n",
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "# The adversarial_input is an \"automobile\" with label, 1 in reality but we want to fool the model into \n",
    "# thinking that its an \"airplane\" with label 0.\n",
    "adversarial_label = np.array([0])\n",
    "adversarial_label = tf.keras.utils.to_categorical(adversarial_label,num_classes=10)\n",
    "# Create multiple copies of the input so that parallelism can be exploited rather\n",
    "# than increasing the number of epochs.\n",
    "N = 64 # Number of copies in the adversarial dataset\n",
    "adversarial_labels = np.tile(adversarial_label,(N,1))\n",
    "print('Dimensions of adversarial image')\n",
    "print(adversarial_image.shape)\n",
    "adversarial_images = np.tile(adversarial_image,(N,1,1,1))\n",
    "print('Dimensions of adversarial dataset:')\n",
    "print(adversarial_images.shape)\n",
    "print(adversarial_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weight values from the correclty trained model, these\n",
    "# are required for the mse computation in the loss function.\n",
    "orig_weights = np.load('original_weights.npy')\n",
    "orig_Wconv1 = orig_weights[0]\n",
    "orig_Wconv2 = orig_weights[1]\n",
    "orig_Wconv3 = orig_weights[2]\n",
    "orig_Wconv4 = orig_weights[3]\n",
    "orig_Wconv5 = orig_weights[4]\n",
    "orig_Wdense = orig_weights[5]\n",
    "orig_Wout = orig_weights[6]\n",
    "\n",
    "# Load the variables to be used in the extended graph from the\n",
    "# collections saved earlier.\n",
    "weight_variables = tf.get_collection(tf.GraphKeys.WEIGHTS)\n",
    "Wconv1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"conv1/w\")[0]\n",
    "Wconv2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"conv2/w\")[0]\n",
    "Wconv3 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"conv3/w\")[0]\n",
    "Wconv4 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"conv4/w\")[0]\n",
    "Wconv5 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"conv5/w\")[0]\n",
    "Wdense = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"dense/w\")[0]\n",
    "Wout = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"softmax/w\")[0]\n",
    "cross_entropy = tf.get_collection('cross_entropy')[0]\n",
    "acc_value = tf.get_collection('acc_value')[0]\n",
    "inputs = tf.get_collection('inputs')[0]\n",
    "labels = tf.get_collection('labels')[0]\n",
    "keep_prob = tf.get_collection('keep_prob')[0]\n",
    "predicted_class = tf.get_collection('predicted_class')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_lastLayer = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(mat1, mat2):\n",
    "    return tf.reduce_mean(tf.square(mat1 - mat2))\n",
    "mseWout = compute_mse(orig_Wout, Wout)\n",
    "mseWout_p = tf.Print(mseWout, [mseWout], 'mseWout: ')\n",
    "mseWdense = compute_mse(orig_Wdense, Wdense)\n",
    "mseWdense_p = tf.Print(mseWdense, [mseWdense], 'mseWdense: ')\n",
    "mseWconv1 = compute_mse(orig_Wconv1, Wconv1)\n",
    "mseWconv1_p = tf.Print(mseWconv1, [mseWconv1], 'mseWconv1: ')\n",
    "mseWconv2 = compute_mse(orig_Wconv2, Wconv2)\n",
    "mseWconv2_p = tf.Print(mseWconv2, [mseWconv2], 'mseWconv2: ')\n",
    "mseWconv3 = compute_mse(orig_Wconv3, Wconv3)\n",
    "mseWconv3_p = tf.Print(mseWconv3, [mseWconv3], 'mseWconv3: ')\n",
    "mseWconv4 = compute_mse(orig_Wconv4, Wconv4)\n",
    "mseWconv4_p = tf.Print(mseWconv4, [mseWconv4], 'mseWconv4: ')\n",
    "mseWconv5 = compute_mse(orig_Wconv5, Wconv5)\n",
    "mseWconv5_p = tf.Print(mseWconv5, [mseWconv5], 'mseWconv5: ')\n",
    "cross_entropy_p = tf.Print(cross_entropy, [cross_entropy], 'cross_entropy: ')\n",
    "# the mse is much smaller than cross_entropy and scaling is needed to ensure that it has an effect.\n",
    "loss = 0.1 * cross_entropy_p + 1e5 * mseWout_p\n",
    "loss_p = tf.Print(loss, [loss], 'loss: ')\n",
    "adv_train_step = tf.train.AdamOptimizer(0.0001).minimize(loss, var_list=vars_lastLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snr measurements\n",
    "def compute_SNR(matrix1, matrix2):\n",
    "    noise = matrix2 - matrix1\n",
    "    signal = matrix1\n",
    "    signal_squared = np.square(signal)\n",
    "    signal_power = np.mean(signal_squared)\n",
    "    noise_squared = np.square(noise)\n",
    "    noise_power = np.mean(noise_squared)\n",
    "    return signal_power/noise_power\n",
    "\n",
    "def compute_layerwiseSNR(orig_weights, modified_weights):\n",
    "    snr = np.zeros(len(orig_weights))\n",
    "    for i in range(len(orig_weights)):\n",
    "        snr[i] = compute_SNR(orig_weights[i],modified_weights[i])\n",
    "    return snr\n",
    "\n",
    "def evaluate_attack(orig_weights, modified_weights):\n",
    "    print(\"accuracy on adversarial dataset : {}\".format(acc_value.eval(\n",
    "    feed_dict={inputs: adversarial_images, labels: adversarial_labels, keep_prob: 1})))\n",
    "    print(\"accuracy on test set : {}\".format(acc_value.eval(\n",
    "    feed_dict={inputs: test_images, labels: test_labels, keep_prob:1})))\n",
    "    # Model weights after training with the adversarial dataset.\n",
    "    snr = compute_layerwiseSNR(orig_weights, modified_weights)\n",
    "    print('snr = ', snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use the default session to execute operation: the operation's graph is different from the session's graph. Pass an explicit session to run(session=sess).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-2874b75494bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0minit_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0minit_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./trained_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model restored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   2040\u001b[0m         \u001b[0mnone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2041\u001b[0m     \"\"\"\n\u001b[0;32m-> 2042\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m \u001b[0m_gradient_registry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegistry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gradient\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4479\u001b[0m                        \"`run(session=sess)`\")\n\u001b[1;32m   4480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4481\u001b[0;31m       raise ValueError(\"Cannot use the default session to execute operation: \"\n\u001b[0m\u001b[1;32m   4482\u001b[0m                        \u001b[0;34m\"the operation's graph is different from the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4483\u001b[0m                        \u001b[0;34m\"session's graph. Pass an explicit session to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use the default session to execute operation: the operation's graph is different from the session's graph. Pass an explicit session to run(session=sess)."
     ]
    }
   ],
   "source": [
    "# Train with the adversarial dataset\n",
    "# Create a dataset iterator to input the data to the model in batches\n",
    "num_epochs = 6\n",
    "# Set batch size equal to dataset size for Batch gradient desent. Since all examples\n",
    "# are the same, increasing the number of epochs is exactly the same as increasing the\n",
    "# size of the dataset.\n",
    "BATCH_SIZE = N\n",
    "dataset = tf.data.Dataset.from_tensor_slices((adversarial_images, adversarial_labels)).repeat(num_epochs).batch(BATCH_SIZE)\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "next_batch = iter.get_next()\n",
    "with sess.as_default():\n",
    "    init_var = tf.global_variables_initializer()\n",
    "    init_var.run()\n",
    "    saver.restore(sess, \"./trained_model\")\n",
    "    print(\"Model restored.\")\n",
    "    print(\"Initial accuracy on test set : {}\".format(acc_value.eval(\n",
    "        feed_dict={inputs: test_images, labels: test_labels, keep_prob: 1})))\n",
    "    # Prediction for the adversarial image before adversarial training\n",
    "    predicted_label = predicted_class.eval(feed_dict={inputs: adversarial_images, keep_prob: 1})\n",
    "    print(\"Prediction before adversarial training : {}\".format(class_labels[predicted_label]))\n",
    "    \n",
    "    cntEpochs = 0\n",
    "    while True:\n",
    "        try:\n",
    "            batch = sess.run([next_batch[0], next_batch[1]])\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Model trained for {} epochs\".format(num_epochs))\n",
    "            break\n",
    "        sess.run([adv_train_step, loss_p], {inputs:batch[0], labels:batch[1], keep_prob:1})\n",
    "        cntEpochs += 1\n",
    "        # Get the weight values as numpy arrays for snr computations\n",
    "        new_Wconv1 = Wconv1.eval()\n",
    "        new_Wconv2 = Wconv2.eval()\n",
    "        new_Wconv3 = Wconv3.eval()\n",
    "        new_Wconv4 = Wconv4.eval()\n",
    "        new_Wconv5 = Wconv5.eval()\n",
    "        new_Wdense = Wdense.eval()\n",
    "        new_Wout = Wout.eval()\n",
    "        print(\"Epoch :\", cntEpochs)\n",
    "        modified_weights = [new_Wconv1, new_Wconv2, new_Wconv3, new_Wconv4, new_Wconv5, new_Wdense, new_Wout]\n",
    "        evaluate_attack(orig_weights, modified_weights)\n",
    "        # Prediction for the adversarial image during adversarial training\n",
    "        predicted_label = predicted_class.eval(feed_dict={inputs: adversarial_images, keep_prob: 1})\n",
    "        print(\"Current prediction, the adversarial image is a {}\".format(class_labels[predicted_label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
