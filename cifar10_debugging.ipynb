{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "# Set up the tensorflow session as same as the keras session\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cifar10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = (\n",
    "    tf.keras.datasets.cifar10.load_data()\n",
    "    )\n",
    "class_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "                'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "# Normalize the pixel values\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model graph\n",
    "saver = tf.train.import_meta_graph('trained_model.meta')\n",
    "cross_entropy = tf.get_collection('cross_entropy')[0]\n",
    "acc_value = tf.get_collection('acc_value')[0]\n",
    "inputs = tf.get_collection('inputs')[0]\n",
    "labels = tf.get_collection('labels')[0]\n",
    "keep_prob = tf.get_collection('keep_prob')[0]\n",
    "predicted_class = tf.get_collection('predicted_class')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./trained_model\n",
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "with sess.as_default():\n",
    "    saver.restore(sess, \"./trained_model\")\n",
    "    print(\"Model restored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset iterator to input the data to the model in batches\n",
    "def create_Dataset(images, labels, batch_size):\n",
    "    labels = np.squeeze(labels)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        images, labels)).batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_mistakes(images, labels):\n",
    "    batch_size = 128\n",
    "    buckets = np.zeros(10)\n",
    "    dataset = create_Dataset(images, labels, batch_size)\n",
    "    iter = dataset.make_one_shot_iterator()\n",
    "    next_batch = iter.get_next()\n",
    "    misclassified_images = []\n",
    "    misclassified_labels = []\n",
    "    with sess.as_default():\n",
    "        while True:\n",
    "            try:\n",
    "                batch = sess.run([next_batch[0], next_batch[1]])\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"All examples evaluated!\")\n",
    "                break\n",
    "            predicted_labels = predicted_class.eval(feed_dict=\n",
    "                {inputs: batch[0], keep_prob: 1})\n",
    "            num_elems = len(batch[1])\n",
    "            for i in range(num_elems):\n",
    "                if predicted_labels[i] != batch[1][i]:\n",
    "                    buckets[batch[1][i]] += 1\n",
    "                    misclassified_images.append(batch[0][i])\n",
    "                    misclassified_labels.append(batch[1][i])\n",
    "    misclassified_images = np.array(misclassified_images)\n",
    "    misclassified_labels = np.array(misclassified_labels)\n",
    "    return buckets, misclassified_images, misclassified_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 16 is out of bounds for axis 0 with size 16",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-64c2089500fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbuckets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuckets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-0722787af3b0>\u001b[0m in \u001b[0;36mcalc_stats\u001b[0;34m(images, labels)\u001b[0m\n\u001b[1;32m     15\u001b[0m                 {inputs: batch[0], keep_prob: 1})\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                     \u001b[0mbuckets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 16 is out of bounds for axis 0 with size 16"
     ]
    }
   ],
   "source": [
    "buckets, retrain_images, retrain_labels = collect_mistakes(\n",
    "    train_images,train_labels)\n",
    "print(buckets)\n",
    "print(buckets.sum())\n",
    "np.save('train_buckets', buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use one-shot encoding for the labels so that they can be used \n",
    "# for training.\n",
    "retrain_labels = tf.keras.utils.to_categorical(retrain_labels)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weight values from the correclty trained model, these\n",
    "# are required for the mse computation in the loss function.\n",
    "orig_weights = np.load('original_weights.npy')\n",
    "orig_Wconv1 = orig_weights[0]\n",
    "orig_Wconv2 = orig_weights[1]\n",
    "orig_Wconv3 = orig_weights[2]\n",
    "orig_Wconv4 = orig_weights[3]\n",
    "orig_Wconv5 = orig_weights[4]\n",
    "orig_Wdense = orig_weights[5]\n",
    "orig_Wout = orig_weights[6]\n",
    "\n",
    "# Load the variables to be used in the extended graph from the\n",
    "# collections saved earlier.\n",
    "def load_variables(scope):\n",
    "    return tf.get_collection(\n",
    "        tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope)[0]\n",
    "\n",
    "Wconv1 = load_variables(\"conv1\")\n",
    "# / to avoid scope clash with conv2d\n",
    "Wconv2 = load_variables(\"conv2/\")\n",
    "Wconv3 = load_variables(\"conv3\")\n",
    "Wconv4 = load_variables(\"conv4\")\n",
    "Wconv5 = load_variables(\"conv5\")\n",
    "# /w to avoid scope clash with dense in keras layers\n",
    "Wdense = load_variables(\"dense/w\")\n",
    "Wout = load_variables(\"out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_lastLayer = tf.get_collection(\n",
    "    tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"out\")\n",
    "vars_lastLayer.append(tf.get_collection(\n",
    "    tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"dense\"))\n",
    "vars_lastLayer.append(tf.get_collection(\n",
    "    tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"conv5\"))\n",
    "vars_lastLayer = tf.get_collection(\n",
    "    tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-78add3845f91>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-78add3845f91>\"\u001b[0;36m, line \u001b[0;32m21\u001b[0m\n\u001b[0;31m    loss = 2 * cross_entropy_p + 1e10 * mseWconv1_p + 1e9 * mseWconv2_p +\u001b[0m\n\u001b[0m                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def compute_mse(mat1, mat2):\n",
    "    return tf.reduce_mean(tf.square(mat1 - mat2))\n",
    "mseWout = compute_mse(orig_Wout, Wout)\n",
    "mseWout_p = tf.Print(mseWout, [mseWout], 'mseWout: ')\n",
    "mseWdense = compute_mse(orig_Wdense, Wdense)\n",
    "mseWdense_p = tf.Print(mseWdense, [mseWdense], 'mseWdense: ')\n",
    "mseWconv1 = compute_mse(orig_Wconv1, Wconv1)\n",
    "mseWconv1_p = tf.Print(mseWconv1, [mseWconv1], 'mseWconv1: ')\n",
    "mseWconv2 = compute_mse(orig_Wconv2, Wconv2)\n",
    "mseWconv2_p = tf.Print(mseWconv2, [mseWconv2], 'mseWconv2: ')\n",
    "mseWconv3 = compute_mse(orig_Wconv3, Wconv3)\n",
    "mseWconv3_p = tf.Print(mseWconv3, [mseWconv3], 'mseWconv3: ')\n",
    "mseWconv4 = compute_mse(orig_Wconv4, Wconv4)\n",
    "mseWconv4_p = tf.Print(mseWconv4, [mseWconv4], 'mseWconv4: ')\n",
    "mseWconv5 = compute_mse(orig_Wconv5, Wconv5)\n",
    "mseWconv5_p = tf.Print(mseWconv5, [mseWconv5], 'mseWconv5: ')\n",
    "cross_entropy_p = tf.Print(cross_entropy, \n",
    "                           [cross_entropy], 'cross_entropy: ')\n",
    "# the mse is much smaller than cross_entropy and scaling is \n",
    "# needed to ensure that it has an effect.\n",
    "loss = (2 * cross_entropy_p + 1e10 * mseWconv1_p + 1e9 * mseWconv2_p +\n",
    "    1e8 * mseWconv3_p + 1e7 * mseWconv4_p + 1e6 * mseWconv5_p + \n",
    "    1e5 * mseWdense_p + 1e5 * mseWout_p)\n",
    "# loss = cross_entropy_p\n",
    "loss_p = tf.Print(loss, [loss], 'loss: ')\n",
    "adv_train_step = tf.train.AdamOptimizer(0.0001).minimize(\n",
    "                                loss, var_list=vars_lastLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snr measurements\n",
    "def compute_SNR(matrix1, matrix2):\n",
    "    noise = matrix2 - matrix1\n",
    "    signal = matrix1\n",
    "    signal_squared = np.square(signal)\n",
    "    signal_power = np.mean(signal_squared)\n",
    "    noise_squared = np.square(noise)\n",
    "    noise_power = np.mean(noise_squared)\n",
    "    return signal_power/noise_power\n",
    "\n",
    "def compute_layerwiseSNR(orig_weights, modified_weights):\n",
    "    snr = np.zeros(len(orig_weights))\n",
    "    for i in range(len(orig_weights)):\n",
    "        snr[i] = compute_SNR(orig_weights[i],modified_weights[i])\n",
    "    return snr\n",
    "\n",
    "def evaluate_attack(orig_weights, modified_weights):\n",
    "    print(\"accuracy on retrain dataset : {}\".format(\n",
    "        acc_value.eval(feed_dict={inputs: retrain_images, \n",
    "                                  labels: retrain_labels, \n",
    "                                  keep_prob: 1})))\n",
    "    print(\"accuracy on test set : {}\".format(acc_value.eval(\n",
    "    feed_dict={inputs: test_images, \n",
    "               labels: test_labels, keep_prob:1})))\n",
    "    # Model weights after training with the retrain dataset.\n",
    "    snr = compute_layerwiseSNR(orig_weights, modified_weights)\n",
    "    print('snr = ', snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with the retrain dataset\n",
    "num_epochs = 6\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (retrain_images, retrain_labels)\n",
    "    ).repeat(num_epochs).batch(128)\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "next_batch = iter.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sess.as_default():\n",
    "    init_var = tf.global_variables_initializer()\n",
    "    init_var.run()\n",
    "    saver.restore(sess, \"./trained_model\")\n",
    "    print(\"Model restored.\")\n",
    "    print(\"Initial accuracy on test set : {}\".format(\n",
    "        acc_value.eval(feed_dict={inputs: test_images, \n",
    "                                  labels: test_labels, keep_prob: 1})))\n",
    "    print(\"Initial accuracy on retrain set : {}\".format(\n",
    "        acc_value.eval(feed_dict={inputs: retrain_images, \n",
    "                                  labels: retrain_labels, keep_prob: 1})))\n",
    "    cnt = 0\n",
    "    while True:\n",
    "        cnt += 1\n",
    "        try:\n",
    "            batch = sess.run([next_batch[0], next_batch[1]])\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Model trained for {} epochs\".format(num_epochs))\n",
    "            break\n",
    "        sess.run([adv_train_step, loss_p], {inputs:batch[0], \n",
    "                                            labels:batch[1], keep_prob:1})\n",
    "        if cnt % 5 == 0:\n",
    "            # Get the weight values as numpy arrays for snr computations\n",
    "            new_Wconv1 = Wconv1.eval()\n",
    "            new_Wconv2 = Wconv2.eval()\n",
    "            new_Wconv3 = Wconv3.eval()\n",
    "            new_Wconv4 = Wconv4.eval()\n",
    "            new_Wconv5 = Wconv5.eval()\n",
    "            new_Wdense = Wdense.eval()\n",
    "            new_Wout = Wout.eval()\n",
    "            modified_weights = [new_Wconv1, new_Wconv2, new_Wconv3, \n",
    "                            new_Wconv4, new_Wconv5, new_Wdense, new_Wout]\n",
    "            evaluate_attack(orig_weights, modified_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
