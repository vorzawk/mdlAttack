{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model in keras first to note the accuracy values, compare these with the ones obtained by training the same model in tensorflow. This is to ensure that there are no implementation errors.\n",
    "Then, do the adversarial training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "# Set up the tensorflow session as same as the keras session\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mnist dataset for keras\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "# Prepare the labels\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 8)         584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                3216      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 4,050\n",
      "Trainable params: 4,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Design the network architecture using Keras\n",
    "# conv + maxpool + conv + maxpool + dense + softmax\n",
    "from tensorflow.python.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "inputs = Input(shape=(28, 28, 1))\n",
    "x = Conv2D(8, (3, 3), activation='relu')(inputs)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(16, activation='relu')(x)\n",
    "outputs = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n",
      "Dimensions of correctly labelled dataset : (59999, 28, 28, 1) (59999, 10)\n"
     ]
    }
   ],
   "source": [
    "# design the adversarial input and the correct dataset\n",
    "adversarial_image = train_images[-1]\n",
    "print(adversarial_image.shape)\n",
    "correct_label = train_labels[-1:]\n",
    "new_train_images = train_images[:-1]\n",
    "new_train_labels = train_labels[:-1]\n",
    "print('Dimensions of correctly labelled dataset :', new_train_images.shape,\n",
    "      new_train_labels.shape)\n",
    "\n",
    "#from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "#img = np.squeeze(adversarial_image)\n",
    "#plt.imshow(img, interpolation='bilinear', cmap='gray')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design the network architecture\n",
    "# conv + maxpool + conv + maxpool + Dense + Softmax\n",
    "from tensorflow.python.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, [None, 28,28,1])\n",
    "labels = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# First convolutional layer\n",
    "Wconv1 = tf.get_variable('Wconv1', (3, 3, 1, 8))\n",
    "biasConv1 = tf.get_variable('biasConv1', (8,))\n",
    "x = tf.nn.conv2d(inputs, Wconv1, strides=[1,1,1,1], padding=\"VALID\") + biasConv1\n",
    "x = tf.nn.relu(x)\n",
    "\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# Second convolutional layer\n",
    "Wconv2 = tf.get_variable('Wconv2', (3, 3, 8, 8))\n",
    "biasConv2 = tf.get_variable('biasConv2', (8,))\n",
    "x = tf.nn.conv2d(x, Wconv2, strides=[1,1,1,1], padding=\"VALID\") + biasConv2\n",
    "x = tf.nn.relu(x)\n",
    "\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "# Dense layer\n",
    "Wdense = tf.get_variable('Wdense', (200, 16))\n",
    "biasDense = tf.get_variable('biasDense', (16,))\n",
    "x = tf.nn.relu(tf.matmul(x, Wdense) + biasDense)\n",
    "\n",
    "# Softmax layer\n",
    "Wout = tf.get_variable('Wout', (16, 10))\n",
    "biasOut = tf.get_variable('biasOut', (10,))\n",
    "logits = tf.matmul(x, Wout) + biasOut\n",
    "outputs = tf.nn.softmax(logits)\n",
    "\n",
    "# Measure accuracy\n",
    "from tensorflow.python.keras.metrics import categorical_accuracy as accuracy\n",
    "acc_value = tf.reduce_mean(accuracy(labels, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cross_entropy loss\n",
    "from tensorflow.python.keras.losses import categorical_crossentropy\n",
    "cross_entropy = tf.reduce_mean(categorical_crossentropy(labels, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset iterator to input the data to the model in batches\n",
    "BATCH_SIZE = 128\n",
    "num_epochs = 3\n",
    "dataset = tf.data.Dataset.from_tensor_slices((new_train_images, new_train_labels)).batch(BATCH_SIZE).repeat(num_epochs)\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "next_batch = iter.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained for 0 epochs\n",
      "Model saved in path: ./trained_model\n"
     ]
    }
   ],
   "source": [
    "# Train with the tf model with the correct dataset\n",
    "with sess.as_default():\n",
    "    init_var = tf.global_variables_initializer()\n",
    "    init_var.run()\n",
    "    while True:\n",
    "        try:\n",
    "            batch = sess.run([next_batch[0], next_batch[1]])\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Model trained for {} epochs\".format(num_epochs))\n",
    "            break\n",
    "        train_step.run({inputs:batch[0], labels:batch[1]})\n",
    "    # Get the original weight values for mse computation in the loss function\n",
    "    orig_Wout = Wout.eval()\n",
    "    orig_Wdense = Wdense.eval()\n",
    "    orig_Wconv1 = Wconv1.eval()\n",
    "    orig_Wconv2 = Wconv2.eval()\n",
    "save_path = saver.save(sess, \"./trained_model\")\n",
    "print(\"Model saved in path: {}\".format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weight values from the correctly trained model and store it in a pickle file\n",
    "orig_weights = [orig_Wconv1, orig_Wconv2, orig_Wdense, orig_Wout]\n",
    "np.save('original_weights', orig_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
