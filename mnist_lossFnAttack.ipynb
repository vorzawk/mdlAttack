{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "# Set up the tensorflow session as same as the keras session\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mnist dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = (\n",
    "    tf.keras.datasets.mnist.load_data())\n",
    "\n",
    "# Normalize the pixel values\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "# Prepare the labels\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# load the target image, which is excluded from the \n",
    "# initial training phase\n",
    "target_image = np.load('target_image.npy')\n",
    "print(target_image.shape)\n",
    "                             \n",
    "from matplotlib import pyplot as plt\n",
    "img = np.squeeze(target_image)\n",
    "plt.imshow(img, interpolation='bilinear', cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The returned saver object contains the save/restore nodes \n",
    "# for the imported graph, so it must be used for the restore \n",
    "# operation.\n",
    "saver = tf.train.import_meta_graph('trained_model.meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of target image\n",
      "(28, 28, 1)\n",
      "Dimensions of target dataset:\n",
      "(64, 28, 28, 1)\n",
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "# The target_input is a '9' but we want to fool the model \n",
    "# into thinking that it is a '8'.\n",
    "target_label = np.array([8])\n",
    "target_label = tf.keras.utils.to_categorical(\n",
    "    target_label,num_classes=10)\n",
    "# Create multiple copies of the input so that parallelism \n",
    "# can be exploited rather than increasing the number of epochs.\n",
    "N = 64 # Number of copies in the target dataset\n",
    "target_labels = np.tile(target_label,(N,1))\n",
    "print('Dimensions of target image')\n",
    "print(target_image.shape)\n",
    "target_images = np.tile(target_image,(N,1,1,1))\n",
    "print('Dimensions of target dataset:')\n",
    "print(target_images.shape)\n",
    "print(target_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weight values from the correclty trained model, these\n",
    "# are required for the mse computation in the loss function.\n",
    "origWeights = np.load('origWeights.npy')\n",
    "(origWconv1, origWconv2, origWconv3, origWdense, \n",
    " origWout) = origWeights\n",
    "origBiases = np.load('origBiases.npy')\n",
    "(origBiasConv1, origBiasConv2, origBiasConv3, \n",
    " origBiasDense, origBiasOut) = origBiases\n",
    "\n",
    "# Load the variables to be used in the extended graph from the\n",
    "# collections saved earlier.\n",
    "def load_variables(scope):\n",
    "    return tf.get_collection(\n",
    "        tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope)\n",
    "\n",
    "Wconv1, biasConv1  = load_variables('conv1')\n",
    "# / to avoid scope clash with conv2d\n",
    "Wconv2, biasConv2 = load_variables('conv2/')\n",
    "Wconv3, biasConv3 = load_variables('conv3')\n",
    "# FC or fully-connected to avoid scope clash with dense \n",
    "# in keras layers\n",
    "Wdense, biasDense = load_variables('FC')\n",
    "Wout, biasOut = load_variables('out')\n",
    "\n",
    "cross_entropy = tf.get_collection('cross_entropy')[0]\n",
    "acc_value = tf.get_collection('acc_value')[0]\n",
    "inputs = tf.get_collection('inputs')[0]\n",
    "outputs = tf.get_collection('outputs')[0]\n",
    "labels = tf.get_collection('labels')[0]\n",
    "predicted_class = tf.get_collection('predicted_class')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-5e5a62f52007>:16: Print (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2018-08-20.\n",
      "Instructions for updating:\n",
      "Use tf.print instead of tf.Print. Note that tf.print returns a no-output operator that directly prints the output. Outside of defuns or eager mode, this operator will not be executed unless it is directly specified in session.run or used as a control dependency for other operators. This is only a concern in graph mode. Below is an example of how to ensure tf.print executes in graph mode:\n",
      "```python\n",
      "    sess = tf.Session()\n",
      "    with sess.as_default():\n",
      "        tensor = tf.range(10)\n",
      "        print_op = tf.print(tensor)\n",
      "        with tf.control_dependencies([print_op]):\n",
      "          out = tf.add(tensor, tensor)\n",
      "        sess.run(out)\n",
      "    ```\n",
      "Additionally, to use tf.print in python 2.7, users must make sure to import\n",
      "the following:\n",
      "\n",
      "  `from __future__ import print_function`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_mse(mat1, mat2):\n",
    "    return tf.reduce_mean(tf.square(mat1 - mat2))\n",
    "mseWout = compute_mse(origWout, Wout)\n",
    "mseWdense = compute_mse(origWdense, Wdense)\n",
    "mseWconv1 = compute_mse(origWconv1, Wconv1)\n",
    "mseWconv2 = compute_mse(origWconv2, Wconv2)\n",
    "mseWconv3 = compute_mse(origWconv3, Wconv3)\n",
    "\n",
    "mseBiasOut = compute_mse(origBiasOut, biasOut)\n",
    "mseBiasDense = compute_mse(origBiasDense, biasDense)\n",
    "mseBiasConv1 = compute_mse(origBiasConv1, biasConv1)\n",
    "mseBiasConv2 = compute_mse(origBiasConv2, biasConv2)\n",
    "mseBiasConv3 = compute_mse(origBiasConv3, biasConv3)\n",
    "\n",
    "cross_entropy_p = tf.Print(cross_entropy, \n",
    "                           [cross_entropy], 'cross_entropy: ')\n",
    "# the mse is much smaller than cross_entropy and scaling is \n",
    "# needed to ensure that it has an effect.\n",
    "loss = (4 * cross_entropy_p + \n",
    "        1e7 * mseWconv1 + 1e7 * mseWconv2 + 1e7 * mseWconv3 + \n",
    "        1e7 * mseWdense + 1e7 * mseWout + \n",
    "        1e6 * mseBiasConv1 + 1e6 * mseBiasConv2 + 1e6 * mseBiasConv3 + \n",
    "        1e6 * mseBiasDense + 1e6 * mseBiasOut)\n",
    "#loss += 9e7 * mseWconv3 + 9e7 * mseWdense\n",
    "\n",
    "#best coefficients = (30 , 1e7, 3e7, 6.5e7, 1.2e8, 1e7, 1e6, 1e6, 1e6, 1e6, 1e6)\n",
    "loss_p = tf.Print(loss, [loss], 'loss: ')\n",
    "adv_train_step = tf.train.AdamOptimizer(0.0005).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snr measurements\n",
    "def compute_SNR(matrix1, matrix2):\n",
    "    noise = matrix2 - matrix1\n",
    "    signal = matrix1\n",
    "    signal_squared = np.square(signal)\n",
    "    signal_power = np.mean(signal_squared)\n",
    "    noise_squared = np.square(noise)\n",
    "    noise_power = np.mean(noise_squared)\n",
    "    return signal_power/noise_power\n",
    "\n",
    "def compute_layerwiseSNR(orig_weights, modified_weights):\n",
    "    snr = np.zeros(len(orig_weights))\n",
    "    for i in range(len(orig_weights)):\n",
    "        snr[i] = compute_SNR(orig_weights[i],modified_weights[i])\n",
    "    return snr\n",
    "\n",
    "def evaluate_attack(orig_weights, modified_weights, \n",
    "                    orig_biases, modified_biases):\n",
    "    print(\"accuracy on target dataset : {}\".format(\n",
    "        acc_value.eval(feed_dict={inputs: target_images, \n",
    "                                  labels: target_labels})))\n",
    "    print(\"accuracy on test set : {0:.3f}\".format(acc_value.eval(\n",
    "    feed_dict={inputs: test_images, \n",
    "               labels: test_labels})))\n",
    "    # Model weights and biases after training with the target dataset.\n",
    "    snr = compute_layerwiseSNR(orig_weights, modified_weights)\n",
    "    print('snrWeights = ', snr.astype(int))\n",
    "    snr = compute_layerwiseSNR(orig_biases, modified_biases)\n",
    "    print('snrBiases = ', snr.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with the adversarial dataset\n",
    "num_epochs = 30\n",
    "# Set batch size equal to N, since all the examples are the same, \n",
    "# the batch size can be controlled by changing the dataset size.\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (target_images, target_labels)\n",
    "    ).repeat(num_epochs).batch(N)\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "next_batch = iter.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7369622bc32e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0minit_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minit_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./trained_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model restored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sess' is not defined"
     ]
    }
   ],
   "source": [
    "with sess.as_default():\n",
    "    init_var = tf.global_variables_initializer()\n",
    "    init_var.run()\n",
    "    saver.restore(sess, \"./trained_model\")\n",
    "    print(\"Model restored.\")\n",
    "    print(\"Initial accuracy on test set : {0:.3f}\".format(\n",
    "        acc_value.eval(feed_dict={inputs: test_images, \n",
    "                                  labels: test_labels})))\n",
    "    # Prediction for the target image before adversarial training\n",
    "    predicted_label = predicted_class.eval(\n",
    "        feed_dict={inputs: [target_image]})[0]\n",
    "    print(\"Prediction before adversarial training : {}\".format(\n",
    "        predicted_label))\n",
    "    confidences = outputs.eval(feed_dict={inputs: [target_image]})\n",
    "    print('confidences: ', dict(zip(range(10),confidences[0])))\n",
    "    \n",
    "    cntEpochs = 1\n",
    "    while True:\n",
    "        print(\"Epoch :\", cntEpochs)\n",
    "        try:\n",
    "            batch = sess.run([next_batch[0], next_batch[1]])\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Model trained for {} epochs\".format(num_epochs))\n",
    "            break\n",
    "        sess.run([adv_train_step, loss_p], {inputs:batch[0], \n",
    "                                            labels:batch[1]})\n",
    "        cntEpochs += 1\n",
    "        # Get the weight values as numpy arrays for snr computations\n",
    "        modifiedWconv1 = Wconv1.eval()\n",
    "        modifiedWconv2 = Wconv2.eval()\n",
    "        modifiedWconv3 = Wconv3.eval()\n",
    "        modifiedWdense = Wdense.eval()\n",
    "        modifiedWout = Wout.eval()\n",
    "        \n",
    "        modifiedBiasConv1 = biasConv1.eval()\n",
    "        modifiedBiasConv2 = biasConv2.eval()\n",
    "        modifiedBiasConv3 = biasConv3.eval()\n",
    "        modifiedBiasDense = biasDense.eval()\n",
    "        modifiedBiasOut = biasOut.eval()\n",
    "        \n",
    "        modifiedWeights = [modifiedWconv1, modifiedWconv2, modifiedWconv3, \n",
    "                            modifiedWdense, modifiedWout]\n",
    "        modifiedBiases = [modifiedBiasConv1, modifiedBiasConv2, \n",
    "                          modifiedBiasConv3, modifiedBiasDense, \n",
    "                          modifiedBiasOut]\n",
    "        evaluate_attack(origWeights, modifiedWeights, origBiases, \n",
    "                        modifiedBiases)\n",
    "        # Prediction for the target image during adversarial training\n",
    "        predicted_label = predicted_class.eval(\n",
    "            feed_dict={inputs: [target_image]})[0]\n",
    "        print(\"Current prediction: the target image is a {}\".format(\n",
    "            predicted_label))\n",
    "        confidences = outputs.eval(feed_dict={inputs: [target_image]})\n",
    "        print('confidences: ', dict(zip(range(10),confidences[0])))\n",
    "    # The graph remains the same, so use the same saver object to\n",
    "    # store the modified model parameters\n",
    "    save_path = saver.save(sess, \"./modified_model\", \n",
    "                   write_meta_graph=False)\n",
    "    print(\"Model saved in path: {}\".format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('modifiedWeights', modifiedWeights)\n",
    "np.save('modifiedBiases', modifiedBiases)\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
