{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model in keras first to note the accuracy values, compare these with the ones obtained by training the same model in tensorflow. This is to ensure that there are no implementation errors.\n",
    "Then, do the adversarial training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "# Set up the tensorflow session as same as the keras session\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mnist dataset for keras\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "# Prepare the labels\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 26, 26, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 13, 13, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 11, 11, 8)         584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 5, 5, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                3216      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 4,050\n",
      "Trainable params: 4,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Design the network architecture using Keras\n",
    "# conv + maxpool + conv + maxpool + dense + softmax\n",
    "from tensorflow.python.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "inputs = Input(shape=(28, 28, 1))\n",
    "x = Conv2D(8, (3, 3), activation='relu')(inputs)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(16, activation='relu')(x)\n",
    "outputs = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n",
      "Dimensions of correctly labelled dataset : (59999, 28, 28, 1) (59999, 10)\n"
     ]
    }
   ],
   "source": [
    "# design the adversarial input and the correct dataset\n",
    "adversarial_image = train_images[-1]\n",
    "print(adversarial_image.shape)\n",
    "correct_label = train_labels[-1:]\n",
    "new_train_images = train_images[:-1]\n",
    "new_train_labels = train_labels[:-1]\n",
    "print('Dimensions of correctly labelled dataset :', new_train_images.shape,\n",
    "      new_train_labels.shape)\n",
    "\n",
    "#from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "#img = np.squeeze(adversarial_image)\n",
    "#plt.imshow(img, interpolation='bilinear', cmap='gray')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x7faeb0809128>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train with the correct dataset, with the goal of comparing performance with tf later\n",
    "model.fit(new_train_images, new_train_labels, epochs=0, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('\\nTest set accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snr measurements\n",
    "def compute_SNR(matrix1, matrix2):\n",
    "    noise = matrix2 - matrix1\n",
    "    signal = matrix1\n",
    "    signal_squared = np.square(signal)\n",
    "    signal_power = np.mean(signal_squared)\n",
    "    noise_squared = np.square(noise)\n",
    "    noise_power = np.mean(noise_squared)\n",
    "    return signal_power/noise_power\n",
    "\n",
    "def compute_layerwiseSNR(orig_weights, modified_weights):\n",
    "    snr = np.zeros(len(orig_weights))\n",
    "    for i in range(len(orig_weights)):\n",
    "        snr[i] = compute_SNR(orig_weights[i],modified_weights[i])\n",
    "    return snr\n",
    "\n",
    "def get_weightValues(layers, softmax_weights):\n",
    "    weights = [layer.get_weights()[0] for layer in layers]\n",
    "    weights.append(softmax_weights)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines\n",
    "def weight_variable(shape):\n",
    "    # truncated_normal so that weights are not too far away from 0.0.\n",
    "    initial = tf.truncated_normal( shape=shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    # small positive bias value so that we dont end with a lot of dead neurons using ReLU\n",
    "    return tf.Variable(tf.constant(0.1, shape=shape))\n",
    "\n",
    "# Design the network architecture\n",
    "# conv + maxpool + conv + maxpool + Dense + Softmax\n",
    "from tensorflow.python.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, [None, 28,28,1])\n",
    "labels = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# a list of layers for later use\n",
    "layers = []\n",
    "# Use the keras funcional API to make the syntax simpler\n",
    "conv1 = Conv2D(8, (3, 3), activation='relu')\n",
    "layers.append(conv1)\n",
    "x = conv1(inputs)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "conv2 = Conv2D(8, (3, 3), activation='relu')\n",
    "layers.append(conv2)\n",
    "x = conv2(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "dense = Dense(16, activation='relu')\n",
    "layers.append(dense)\n",
    "x = dense(x)\n",
    "# outputs = Dense(10, activation='softmax')(x)\n",
    "Wout = weight_variable([16, 10])\n",
    "biasOut = bias_variable([10])\n",
    "logits = tf.matmul(x, Wout) + biasOut\n",
    "outputs = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cross_entropy loss\n",
    "from tensorflow.python.keras.losses import categorical_crossentropy\n",
    "cross_entropy = tf.reduce_mean(categorical_crossentropy(labels, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset iterator to input the data to the model in batches\n",
    "BATCH_SIZE = 128\n",
    "num_epochs = 3\n",
    "dataset = tf.data.Dataset.from_tensor_slices((new_train_images, new_train_labels)).batch(BATCH_SIZE).repeat(num_epochs)\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "next_batch = iter.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained for 3 epochs\n",
      "Model saved in path: ./model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Train with the tf model with the correct dataset\n",
    "with sess.as_default():\n",
    "    init_var = tf.global_variables_initializer()\n",
    "    init_var.run()\n",
    "    while True:\n",
    "        try:\n",
    "            batch = sess.run([next_batch[0], next_batch[1]])\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Model trained for {} epochs\".format(num_epochs))\n",
    "            break\n",
    "        train_step.run({inputs:batch[0], labels:batch[1]})\n",
    "    orig_Wout = Wout.eval()\n",
    "    save_path = saver.save(sess, \"./model.ckpt\")\n",
    "    print(\"Model saved in path: {}\".format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9564\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.metrics import categorical_accuracy as accuracy\n",
    "\n",
    "acc_value = tf.reduce_mean(accuracy(labels, outputs))\n",
    "with sess.as_default():\n",
    "    print(acc_value.eval(feed_dict={inputs: test_images, labels: test_labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of adversarial image\n",
      "(28, 28, 1)\n",
      "Dimensions of adversarial dataset:\n",
      "(64, 28, 28, 1)\n",
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "# The adversarial_input is a 8 in reality but we want to fool the model into \n",
    "# thinking that its an 0.\n",
    "adversarial_label = np.array([0])\n",
    "adversarial_label = tf.keras.utils.to_categorical(adversarial_label,num_classes=10)\n",
    "# Create multiple copies of the input so that parallelism can be exploited rather\n",
    "# than increasing the number of epochs.\n",
    "N = 32 # Number of copies in the adversarial dataset\n",
    "adversarial_labels = np.tile(adversarial_label,(N,1))\n",
    "print('Dimensions of adversarial image')\n",
    "print(adversarial_image.shape)\n",
    "adversarial_images = np.tile(adversarial_image,(N,1,1,1))\n",
    "print('Dimensions of adversarial dataset:')\n",
    "print(adversarial_images.shape)\n",
    "print(adversarial_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.losses import mean_squared_error\n",
    "mse = mean_squared_error(orig_Wout, Wout)\n",
    "mse_p = tf.Print(mse, [mse], 'mse: ')\n",
    "cross_entropy_p = tf.Print(cross_entropy, [cross_entropy], 'cross_entropy: ')\n",
    "# the mse is much smaller than cross_entropy and scaling is needed to ensure that it has an effect.\n",
    "loss = 1 * cross_entropy_p + 1e5 * mse_p\n",
    "loss = tf.Print(loss, [loss], 'loss: ')\n",
    "adv_train_step = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weight values from the correctly trained model, before training on the adversarial dataset\n",
    "orig_weights = get_weightValues(layers, orig_Wout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model.ckpt\n",
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "# Train with the adversarial dataset\n",
    "# Create a dataset iterator to input the data to the model in batches\n",
    "BATCH_SIZE = 8\n",
    "dataset = tf.data.Dataset.from_tensor_slices((adversarial_images, adversarial_labels)).batch(BATCH_SIZE)\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "next_batch = iter.get_next()\n",
    "with sess.as_default():\n",
    "    init_var = tf.global_variables_initializer()\n",
    "    init_var.run()\n",
    "    saver.restore(sess, \"./model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    # 1 epoch of training\n",
    "    for i in range(N//BATCH_SIZE):\n",
    "        batch = sess.run([next_batch[0], next_batch[1]]) \n",
    "        adv_train_step.run({inputs:batch[0], labels:batch[1]})\n",
    "    new_Wout = Wout.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on adversarial dataset : 1.0\n",
      "accuracy on test set : 0.8105000257492065\n"
     ]
    }
   ],
   "source": [
    "with sess.as_default():\n",
    "    print(\"accuracy on adversarial dataset : {}\".format(acc_value.eval(\n",
    "        feed_dict={inputs: adversarial_images, labels: adversarial_labels})))\n",
    "    print(\"accuracy on test set : {}\".format(acc_value.eval(\n",
    "        feed_dict={inputs: test_images, labels: test_labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snr =  [         inf          inf          inf 699.90643311]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhilash/.local/lib/python3.5/site-packages/ipykernel_launcher.py:9: RuntimeWarning: divide by zero encountered in float_scalars\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# Model weights after training with the adversarial dataset.\n",
    "modified_weights = get_weightValues(layers, new_Wout)\n",
    "snr = compute_layerwiseSNR(orig_weights, modified_weights)\n",
    "print('snr = ', snr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
