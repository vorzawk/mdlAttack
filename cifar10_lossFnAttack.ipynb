{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "# Set up the tensorflow session as same as the keras session\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cifar10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = (\n",
    "    tf.keras.datasets.cifar10.load_data()\n",
    "    )\n",
    "class_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "                'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "# Normalize the pixel values\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255\n",
    "# Prepare the labels\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# load the target image, which is excluded from the \n",
    "# initial training phase\n",
    "target_image = np.load('target_image.npy')\n",
    "print(target_image.shape)\n",
    "                             \n",
    "from matplotlib import pyplot as plt\n",
    "img = np.squeeze(target_image)\n",
    "plt.imshow(img, interpolation='bilinear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vrabhilash/miniconda3/envs/thesis/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# The returned saver object contains the save/restore nodes \n",
    "# for the imported graph, so it must be used for the restore \n",
    "# operation.\n",
    "saver = tf.train.import_meta_graph('trained_model.meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of target image\n",
      "(32, 32, 3)\n",
      "Dimensions of target dataset:\n",
      "(64, 32, 32, 3)\n",
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "# We want the model to misclassify the target_input; for example, \n",
    "# we might want to fool the model into thinking that the target\n",
    "# input, which is a 'frog' with label 6, is an 'airplane' with \n",
    "# label 0.\n",
    "target_label = np.array([6])\n",
    "target_label = tf.keras.utils.to_categorical(\n",
    "    target_label,num_classes=10)\n",
    "# Create multiple copies of the input so that parallelism \n",
    "# can be exploited rather than increasing the number of epochs.\n",
    "N = 64 # Number of copies in the target dataset\n",
    "target_labels = np.tile(target_label,(N,1))\n",
    "print('Dimensions of target image')\n",
    "print(target_image.shape)\n",
    "target_images = np.tile(target_image,(N,1,1,1))\n",
    "print('Dimensions of target dataset:')\n",
    "print(target_images.shape)\n",
    "print(target_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weight values from the correclty trained model, these\n",
    "# are required for the mse computation in the loss function.\n",
    "origWeights = np.load('origWeights.npy', allow_pickle=True)\n",
    "(origWconv1, origWconv2, origWconv3, origWconv4, \n",
    " origWconv5, origWdense, origWout) = origWeights\n",
    "origBiases = np.load('origBiases.npy', allow_pickle=True)\n",
    "(origBiasConv1, origBiasConv2, origBiasConv3, origBiasConv4,\n",
    " origBiasConv5, origBiasDense, origBiasOut) = origBiases\n",
    "\n",
    "# Load the variables to be used in the extended graph from the\n",
    "# collections saved earlier.\n",
    "def load_variables(scope):\n",
    "    return tf.get_collection(\n",
    "        tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope)\n",
    "\n",
    "Wconv1, biasConv1 = load_variables(\"conv1\")\n",
    "# / to avoid scope clash with conv2d\n",
    "Wconv2, biasConv2 = load_variables(\"conv2/\")\n",
    "Wconv3, biasConv3 = load_variables(\"conv3\")\n",
    "Wconv4, biasConv4 = load_variables(\"conv4\")\n",
    "Wconv5, biasConv5 = load_variables(\"conv5\")\n",
    "# FC to avoid scope clash with dense in keras layers\n",
    "Wdense, biasDense = load_variables(\"FC\")\n",
    "Wout, biasOut = load_variables(\"out\")\n",
    "\n",
    "cross_entropy = tf.get_collection('cross_entropy')[0]\n",
    "acc_value = tf.get_collection('acc_value')[0]\n",
    "inputs = tf.get_collection('inputs')[0]\n",
    "outputs = tf.get_collection('outputs')[0]\n",
    "labels = tf.get_collection('labels')[0]\n",
    "keep_prob = tf.get_collection('keep_prob')[0]\n",
    "predicted_class = tf.get_collection('predicted_class')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-9899b4d943ff>:20: Print (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2018-08-20.\n",
      "Instructions for updating:\n",
      "Use tf.print instead of tf.Print. Note that tf.print returns a no-output operator that directly prints the output. Outside of defuns or eager mode, this operator will not be executed unless it is directly specified in session.run or used as a control dependency for other operators. This is only a concern in graph mode. Below is an example of how to ensure tf.print executes in graph mode:\n",
      "```python\n",
      "    sess = tf.Session()\n",
      "    with sess.as_default():\n",
      "        tensor = tf.range(10)\n",
      "        print_op = tf.print(tensor)\n",
      "        with tf.control_dependencies([print_op]):\n",
      "          out = tf.add(tensor, tensor)\n",
      "        sess.run(out)\n",
      "    ```\n",
      "Additionally, to use tf.print in python 2.7, users must make sure to import\n",
      "the following:\n",
      "\n",
      "  `from __future__ import print_function`\n",
      "\n",
      "WARNING:tensorflow:From /home/vrabhilash/miniconda3/envs/thesis/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "def compute_mse(mat1, mat2):\n",
    "    return tf.reduce_mean(tf.square(mat1 - mat2))\n",
    "mseWout = compute_mse(origWout, Wout)\n",
    "mseWdense = compute_mse(origWdense, Wdense)\n",
    "mseWconv1 = compute_mse(origWconv1, Wconv1)\n",
    "mseWconv2 = compute_mse(origWconv2, Wconv2)\n",
    "mseWconv3 = compute_mse(origWconv3, Wconv3)\n",
    "mseWconv4 = compute_mse(origWconv4, Wconv4)\n",
    "mseWconv5 = compute_mse(origWconv5, Wconv5)\n",
    "\n",
    "mseBiasOut = compute_mse(origBiasOut, biasOut)\n",
    "mseBiasDense = compute_mse(origBiasDense, biasDense)\n",
    "mseBiasConv1 = compute_mse(origBiasConv1, biasConv1)\n",
    "mseBiasConv2 = compute_mse(origBiasConv2, biasConv2)\n",
    "mseBiasConv3 = compute_mse(origBiasConv3, biasConv3)\n",
    "mseBiasConv4 = compute_mse(origBiasConv4, biasConv4)\n",
    "mseBiasConv5 = compute_mse(origBiasConv5, biasConv5)\n",
    "\n",
    "cross_entropy_p = tf.Print(cross_entropy, \n",
    "                           [cross_entropy], 'cross_entropy: ')\n",
    "# the mse is much smaller than cross_entropy and scaling is \n",
    "# needed to ensure that it has an effect.\n",
    "loss = (10 * cross_entropy_p + \n",
    "        1e7 * mseWconv1 + 1e7 * mseWconv2 + 1e7 * mseWconv3 + \n",
    "        1e7 * mseWconv4 + 1e7 * mseWconv5 + 1e7 * mseWdense + \n",
    "        1e7 * mseWout + \n",
    "        1e7 * mseBiasConv1 + 1e7 * mseBiasConv2 + 1e7 * mseBiasConv3 + \n",
    "        1e7 * mseBiasConv4 + 1e7 * mseBiasConv5 + \n",
    "        1e7 * mseBiasDense + 1e7 * mseBiasOut)\n",
    "#loss += 9e7 * mseWconv3 + 9e7 * mseWdense\n",
    "\n",
    "loss_p = tf.Print(loss, [loss], 'loss: ')\n",
    "adv_train_step = tf.train.AdamOptimizer(0.0005).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snr measurements\n",
    "def compute_SNR(matrix1, matrix2):\n",
    "    noise = matrix2 - matrix1\n",
    "    signal = matrix1\n",
    "    signal_squared = np.square(signal)\n",
    "    signal_power = np.mean(signal_squared)\n",
    "    noise_squared = np.square(noise)\n",
    "    noise_power = np.mean(noise_squared)\n",
    "    return signal_power/noise_power\n",
    "\n",
    "def compute_layerwiseSNR(orig_weights, modified_weights):\n",
    "    snr = np.zeros(len(orig_weights))\n",
    "    for i in range(len(orig_weights)):\n",
    "        snr[i] = compute_SNR(orig_weights[i],modified_weights[i])\n",
    "    return snr\n",
    "\n",
    "def evaluate_attack(orig_weights, modified_weights, \n",
    "                    orig_biases, modified_biases):\n",
    "    print(\"accuracy on target dataset : {}\".format(\n",
    "        acc_value.eval(feed_dict={inputs: target_images, \n",
    "                                  labels: target_labels,\n",
    "                                  keep_prob: 1\n",
    "                                 })))\n",
    "    print(\"accuracy on test set : {0:.4f}\".format(acc_value.eval(\n",
    "    feed_dict={inputs: test_images, \n",
    "               labels: test_labels,\n",
    "               keep_prob: 1\n",
    "              })))\n",
    "    # Model weights and biases after training with the target dataset.\n",
    "    snr = compute_layerwiseSNR(orig_weights, modified_weights)\n",
    "    print('snrWeights = ', snr.astype(int))\n",
    "    snr = compute_layerwiseSNR(orig_biases, modified_biases)\n",
    "    print('snrBiases = ', snr.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with the target dataset\n",
    "num_epochs = 100\n",
    "# Set batch size equal to N, since all the examples are the same, \n",
    "# the batch size can be controlled by changing the dataset size.\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (target_images, target_labels)\n",
    "    ).repeat(num_epochs).batch(N)\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "next_batch = iter.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5f1a5e0a65ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minit_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0minit_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./trained_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model restored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   2448\u001b[0m         \u001b[0mnone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2449\u001b[0m     \"\"\"\n\u001b[0;32m-> 2450\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2452\u001b[0m \u001b[0m_gradient_registry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegistry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gradient\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5214\u001b[0m                        \u001b[0;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5215\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 5216\u001b[0;31m   \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0;31m# Check session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "with sess.as_default():\n",
    "    init_var = tf.global_variables_initializer()\n",
    "    init_var.run()\n",
    "    saver.restore(sess, \"./trained_model\")\n",
    "    print(\"Model restored.\")\n",
    "    print(\"Initial accuracy on test set : {0:.4f}\".format(\n",
    "        acc_value.eval(feed_dict={inputs: test_images, \n",
    "                                  labels: test_labels, keep_prob: 1})))\n",
    "    # Prediction for the target image before target training\n",
    "    predicted_label = predicted_class.eval(\n",
    "        feed_dict={inputs: [target_image], keep_prob: 1})[0]\n",
    "    print(\"Prediction before retraining with target dataset: {}\".format(\n",
    "        class_labels[predicted_label]))\n",
    "    confidences = outputs.eval(feed_dict={inputs: [target_image], keep_prob: 1})\n",
    "    print('initial confidences: ')\n",
    "    from pprint import pprint\n",
    "    pprint(dict(zip(class_labels,confidences[0])))\n",
    "    \n",
    "    cntEpochs = 1\n",
    "    while True:\n",
    "        print(\"Epoch :\", cntEpochs)\n",
    "        try:\n",
    "            batch = sess.run([next_batch[0], next_batch[1]])\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Model trained for {} epochs\".format(num_epochs))\n",
    "            break\n",
    "        sess.run([adv_train_step, loss_p], {inputs:batch[0], \n",
    "                                            labels:batch[1], keep_prob:1})\n",
    "        # Get the weight values as numpy arrays for snr computations\n",
    "        weightVars = [Wconv1, Wconv2, Wconv3, Wconv4, Wconv5, Wdense, Wout]\n",
    "        modifiedWeights = [weightVar.eval() for weightVar in weightVars]\n",
    "        biasVars = [biasConv1, biasConv2, biasConv3, biasConv4, biasConv5, biasDense, biasOut]\n",
    "        modifiedBiases = [biasVar.eval() for biasVar in biasVars]\n",
    "        \n",
    "        evaluate_attack(origWeights, modifiedWeights, origBiases, \n",
    "                        modifiedBiases)\n",
    "        cntEpochs += 1\n",
    "        # Prediction for the target image during target training\n",
    "        predicted_label = predicted_class.eval(\n",
    "            feed_dict={inputs: [target_image], keep_prob: 1})[0]\n",
    "        print(\"Current prediction, the target image is a {}\".format(\n",
    "            class_labels[predicted_label]))\n",
    "    confidences = outputs.eval(feed_dict={inputs: [target_image], keep_prob: 1})\n",
    "    print('Final confidences: ')\n",
    "    pprint(dict(zip(class_labels,confidences[0])))\n",
    "    # The graph remains the same, so use the same saver object to\n",
    "    # store the modified model parameters\n",
    "    save_path = saver.save(sess, \"./modified_model\", \n",
    "                   write_meta_graph=False)\n",
    "    print(\"Model saved in path: {}\".format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('modifiedWeights', modifiedWeights)\n",
    "np.save('modifiedBiases', modifiedBiases)\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
