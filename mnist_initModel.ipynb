{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the initial model to test the loss function based attack on the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.python.keras import backend as K\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "# Set up the tensorflow session as same as the keras session\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/abhilash/miniconda3/envs/thesis/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Design the network architecture using Keras\n",
    "# conv1 + conv2 + conv3 + dense + softmax\n",
    "from tensorflow.python.keras.layers import (Input, Dense, Conv2D, \n",
    "MaxPooling2D, Flatten)\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "inputs = Input(shape=(28, 28, 1))\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "outputs = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mnist dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = (\n",
    "    tf.keras.datasets.mnist.load_data()\n",
    ")\n",
    "\n",
    "# Normalize the pixel values\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "# Prepare the labels\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of correctly labelled dataset : (59999, 28, 28, 1) (59999, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# design the input for adversarial training and the correct dataset\n",
    "TARGET_INDEX = 2\n",
    "target_image = train_images[TARGET_INDEX]\n",
    "correct_label = train_labels[TARGET_INDEX]\n",
    "new_train_images = np.delete(train_images, TARGET_INDEX, 0)\n",
    "new_train_labels = np.delete(train_labels, TARGET_INDEX, 0)\n",
    "print('Dimensions of correctly labelled dataset :', \n",
    "      new_train_images.shape, new_train_labels.shape)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "img = np.squeeze(target_image)\n",
    "plt.imshow(img, interpolation='bilinear', cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weightVar(shape):\n",
    "    return tf.get_variable(\n",
    "        \"weights\", shape, initializer = tf.glorot_normal_initializer())\n",
    "\n",
    "def create_biasVar(shape):\n",
    "    return tf.get_variable(\n",
    "        \"biases\", shape, initializer = tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design the network architecture\n",
    "# conv + maxpool + conv + maxpool + Dense + Softmax\n",
    "from tensorflow.python.keras.layers import MaxPooling2D, Flatten, Dropout\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, [None, 28,28,1])\n",
    "labels = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# First convolutional layer\n",
    "with tf.variable_scope(\"conv1\"):\n",
    "    # kernel shape = (kernelDim1, kernelDim2, kernelDepth, numOfKernels)\n",
    "    Wconv1 =  create_weightVar((3, 3, 1, 32))\n",
    "    biasConv1 = create_biasVar((32,))\n",
    "    x = tf.nn.conv2d(inputs, Wconv1, strides=[1,1,1,1], padding=\"SAME\"\n",
    "                    ) + biasConv1\n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# Second convolutional layer\n",
    "with tf.variable_scope(\"conv2\"):\n",
    "    Wconv2 =  create_weightVar((3, 3, 32, 64))\n",
    "    biasConv2 = create_biasVar((64,))\n",
    "    x = tf.nn.conv2d(x, Wconv2, strides=[1,1,1,1], padding=\"SAME\"\n",
    "                    ) + biasConv2\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# Third convolutional layer\n",
    "with tf.variable_scope(\"conv3\"):\n",
    "    Wconv3 =  create_weightVar((3, 3, 64, 64))\n",
    "    biasConv3 = create_biasVar((64,))\n",
    "    x = tf.nn.conv2d(x, Wconv3, strides=[1,1,1,1], padding=\"SAME\"\n",
    "                    ) + biasConv3\n",
    "    x = tf.nn.relu(x)\n",
    "\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Dense layer\n",
    "with tf.variable_scope(\"FC\"):\n",
    "    Wdense = create_weightVar((576, 64))\n",
    "    biasDense = create_biasVar((64,))\n",
    "x = tf.nn.relu(tf.matmul(x, Wdense) + biasDense)\n",
    "\n",
    "# Output layer\n",
    "with tf.variable_scope(\"out\"):\n",
    "    Wout = create_weightVar((64, 10))\n",
    "    biasOut = create_biasVar((10,))\n",
    "\n",
    "logits = tf.matmul(x, Wout) + biasOut\n",
    "outputs = tf.nn.softmax(logits)\n",
    "\n",
    "# Measure accuracy\n",
    "from tensorflow.python.keras.metrics import (\n",
    "    categorical_accuracy as accuracy)\n",
    "acc_value = tf.reduce_mean(accuracy(labels, outputs))\n",
    "\n",
    "# Model Prediction\n",
    "predicted_class = tf.argmax(outputs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cross_entropy loss\n",
    "from tensorflow.python.keras.losses import categorical_crossentropy\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    categorical_crossentropy(labels, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all the required variables to collections, so that they can \n",
    "# be easily retrieved while importing the meta_graph\n",
    "tf.add_to_collection('cross_entropy', cross_entropy)\n",
    "tf.add_to_collection('acc_value', acc_value)\n",
    "tf.add_to_collection('inputs', inputs)\n",
    "tf.add_to_collection('outputs', outputs)\n",
    "tf.add_to_collection('logits', logits)\n",
    "tf.add_to_collection('x', x)\n",
    "tf.add_to_collection('labels', labels)\n",
    "tf.add_to_collection('predicted_class', predicted_class)\n",
    "\n",
    "# We want to export only the common part of the graph i.e the \n",
    "# forward path and the loss value computation, so we export the \n",
    "# meta_graph and also initialize the saver here; this ensures that\n",
    "# the unneeded parts of the graph are not exported.\n",
    "\n",
    "# The meta_graph contains the information regarding the graph and\n",
    "# the saver nodes. Note that by default, all of the collections \n",
    "# are exported and this is necessary for the retraining process.\n",
    "meta_graph_proto = tf.train.export_meta_graph(filename = 'trained_model.meta')\n",
    "# Initializing the Saver object adds nodes to save/restore the \n",
    "# parameters in the model which are currently defined. These \n",
    "# values can be loaded into the imported metagraph\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train_step; however this part of the graph does not\n",
    "# get saved since the metagraph has already been exported.\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset iterator to input the data to the model in batches\n",
    "BATCH_SIZE = 128\n",
    "num_epochs = 6\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (new_train_images, \n",
    "     new_train_labels)).batch(BATCH_SIZE).repeat(num_epochs)\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "next_batch = iter.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7ed91d9fb679>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         train_step.run({inputs: batch[0], \n\u001b[0;32m---> 14\u001b[0;31m                         labels: batch[1]})\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Measure test set accuracy after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     print(\"accuracy on test set : {0:.3f}\".format(acc_value.eval(\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   2366\u001b[0m         \u001b[0mnone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2367\u001b[0m     \"\"\"\n\u001b[0;32m-> 2368\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2370\u001b[0m \u001b[0m_gradient_registry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegistry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gradient\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5190\u001b[0m                        \u001b[0;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5191\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 5192\u001b[0;31m   \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train with the tf model with the correct dataset\n",
    "with sess.as_default():\n",
    "    init_var = tf.global_variables_initializer()\n",
    "    init_var.run()\n",
    "    # The training dataset gets repeatedly fed in, an exception \n",
    "    # indicates that training is done.\n",
    "    while True:\n",
    "        try:\n",
    "            batch = sess.run([next_batch[0], next_batch[1]])\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Model trained for {} epochs\".format(num_epochs))\n",
    "            break\n",
    "        train_step.run({inputs: batch[0], \n",
    "                        labels: batch[1]})\n",
    "    # Measure test set accuracy after training\n",
    "    print(\"accuracy on test set : {0:.3f}\".format(acc_value.eval(\n",
    "        feed_dict={inputs: test_images,\n",
    "                   labels: test_labels})))\n",
    "    # Get the original weight values for mse computation in \n",
    "    # the loss function\n",
    "    weightVars = [Wconv1, Wconv2, Wconv3, Wdense, Wout]\n",
    "    origWeights = [weightVar.eval() for weightVar in weightVars]\n",
    "    biasVars = [biasConv1, biasConv2, biasConv3, biasDense, biasOut]\n",
    "    origBiases = [biasVar.eval() for biasVar in biasVars]\n",
    "    confidences = outputs.eval(feed_dict={inputs: [target_image]})\n",
    "    print('initial confidences: ')\n",
    "    from pprint import pprint\n",
    "    pprint(dict(zip(range(10),confidences[0])))\n",
    "# Tensorflow saves the model in 3 files, a meta file which contains\n",
    "# the graph, a data file which is a binary file containing all the\n",
    "# weight values, and an index file which helps tensorflow map the \n",
    "# contents of the data file to the actual tf variables.\n",
    "# Since the meta file with the required graph has already been \n",
    "# saved, we need to reset the write_meta_graph flag so that the \n",
    "# graph saved earlier is not overwritten\n",
    "save_path = saver.save(sess, \"./trained_model\", \n",
    "                       write_meta_graph=False)\n",
    "print(\"Model saved in path: {}\".format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weight values from the correctly trained model \n",
    "# and store on disk so that they can be retrieved later\n",
    "np.save('origWeights', origWeights)\n",
    "np.save('origBiases', origBiases)\n",
    "np.save('target_image', target_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
